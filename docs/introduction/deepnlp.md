---
layout: default
title: Deep NLP
parent: Introduction
nav_order: 1
---

# 딥러닝 기반 자연어 처리 모델
{: .no_toc }

이 글에서는 딥러닝(Deep Learning) 기반 자연어 처리 모델의 개념과 학습 방법 등을 살펴봅니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---

## 기계의 자연어 처리

컴퓨터는 계산기일 뿐입니다. 자비스 같이 사람 말을 알아듣는 인공지능이 등장하더라도 그 이해의 본질은 계산(computation) 내지 처리(processing)일 뿐입니다. 사람의 자연어 이해(understanding)와는 차이가 있다는 것이지요.

그렇다면 기계가 사람 말을 알아듣는 것처럼 보이게 하려면 어떤 요소들이 있어야 할까요? 우선은 모델(model)이라는 개념부터 소개해 보겠습니다. 모델은 입력을 받아 어떤 태스크를 수행하는 <U>함수(function)</U>입니다.

## **그림1** 모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/vcW4VPS.png" width="400px" title="source: imgur.com" />

그림1에서 확인할 수 있듯 모델의 출력은 확률(probability)이라는 점에 주목할 필요가 있습니다. 확률이란 어떤 사건이 나타날 가능성에 대한 수치이며 그 값은 0에서 1 사이의 범위를 갖습니다. 따라서 모델은 어떤 입력을 받아서 해당 입력이 특정 범주일 확률을 반환하는 <U>확률</U>함수(probability function)입니다.

그렇다면 자연어 처리 모델의 입력은 무엇이 되어야 할까요? 우리의 원래 목적은 사람 말을 알아듣는 인공지능을 만드는 것이었으니 그 입력은 사람 말, 즉 자연어가 되어야 할 것입니다. 다시 말해 자연어 처리 모델은 <U>자연어 입력을 받아서</U> 해당 입력이 특정 범주일 확률을 반환하는 확률함수입니다.

예컨대 우리가 영화 리뷰의 감성(sentiment)을 맞추는 자연어 처리 모델을 만든다고 가정해 봅시다. 그러면 우리가 만든 감성 분석 모델은 수식1처럼 함수 $f$로 써볼 수 있습니다. 이 모델은 모델은 자연어(문장) 입력을 받아 복잡한 내부 계산 과정을 거쳐 해당 문장이 긍정(positive)일 확률, 중립(neural)일 확률, 부정(negative)일 확률을 출력합니다. 

## **수식1** 자연어 처리 모델
{: .no_toc .text-delta }

$$
\begin{align*}
f\left( \text{재미가 없는 편인 영화에요} \right) = [ 0.0, 0.3, 0.7 ] \\
f\left( \text{단언컨대 이 영화 재미 있어요} \right) = [ 1.0, 0.0, 0.0 ]
\end{align*}
$$

2010년대 이후 딥러닝(deep learning)이 각광받으면서 그림2처럼 모델을 딥러닝으로 구성하는 경우가 많아졌습니다. BERT(Bidirectional Encoder Representations from Transformers)나 GPT(Generative Pre-trained Transformer) 같은 모델이 대표적입니다. 이 같은 모델을 <U>딥러닝 기반 자연어 처리 모델</U>이라고 부릅니다. 이 책에서는 주로 딥러닝 기반 자연어 처리 모델을 다룰 것입니다.


## **그림2** 딥러닝 기반 자연어 처리 모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/CPBZIDM.png" width="400px" title="source: imgur.com" />


딥러닝 기반 자연어 처리 모델의 출력 역시 확률입니다. 하지만 사람은 자연어 형태의 출력을 선호합니다. 그것이 이해하기 쉽기도 하고요. 다음과 같이 약간의 후처리(post processing)을 통해 자연어 형태로 바뀌줍니다.

- 재미가 없는 편인 영화에요 → [0.0, 0.3, 0.7] → 이 문장은 부정입니다.
- 단언컨대 이 영화 재미 있어요 → [1.0, 0.0, 0.0] → 이 문장은 긍정입니다.

대부분의 자연어 처리 모델은 자연어 입력을 받아 해당 입력이 특정 범주일 확률을 출력하고, 이 확률값들을 적당히 후처리해서 자연어 형태로 가공해 반환합니다. 문서 분류(document classification), 질의 응답(question answering), 개체명 인식(named entity recognition), 문장 생성(sentence generation) 등 과제가 모두 그렇습니다.

만일 사람 말을 정말 잘 알아듣는 것처럼 보이는 인공지능이 있다면 이 일련의 계산 과정으로 나온 최종 결과가 그럴싸하게 보이기 때문일 것입니다. 많은 연구자와 개발자들이 그럴싸 해보이는 자연어 처리 모델을 만들기 위해 고군분투하고 있습니다.


---

## 딥러닝 모델의 학습

딥러닝 자연어 처리 모델을 만들려면 무엇을 해야 할까요? 우선 데이터부터 준비해야 합니다. 아래처럼 각 문장에 '감성'이라는 꼬리표 혹은 레이블을 달아놓은 자료가 필요합니다. 이를 학습 데이터라고 부릅니다.

## **표1** 감성 학습 데이터
{: .no_toc .text-delta }

|문장|긍정|중립|부정|
|---|---|---|---|
|단언컨대 이 영화 재미 있어요|1|0|0|
|단언컨대 이 영화 재미 없어요|0|0|1|
|...|...|...|...|


그 다음은 모델이 데이터의 패턴(pattern)을 스스로 익히게 해야 합니다. 이를 **학습(train)**이라고 합니다. `단언컨대 이 영화 재미 있어요` 문장을 학습하는 상황이라고 가정해 봅시다. 학습 초기 $f$는 `단언컨대 이 영화 재미 있어요`를 입력 받으면 아래처럼 출력할 겁니다. 문장이 어떤 감성인지 전혀 모르는 상황입니다.


## **그림1** 감성 분석 모델의 출력 (1)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/YUyysLA.jpg" width="250px" title="source: imgur.com" />

그런데 우리는 `단언컨대 이 영화 재미 있어요`라는 문장의 감성, 즉 정답이 $\begin{bmatrix} 1 & 0 & 0 \end{bmatrix}$임을 알고 있습니다. 그런데 현재 모델의 출력(아래 그림의 회색 막대)과 정답을 비교해 보면 중립/부정 점수가 높네요. **깎아야 합니다.** 긍정 점수는 낮네요. **높여 줍니다.** $f$가 `단언컨대 이 영화 재미 있어요`라는 입력을 받았을 때 긍정 점수는 높아지고, 중립/부정 점수는 낮아지도록 모델 전체를 업데이트합니다. 

## **그림2** 감성 분석 모델의 출력 (2)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/nTBwj6u.jpg" width="250px" title="source: imgur.com" />

업데이트를 한번 했는데도 현재 모델의 출력(아래 그림의 회색 막대)과 정답을 비교했을 때 여전히 긍정 점수는 낮습니다. 한번 더 **높여 줍니다.** 중립/부정 점수는 여전히 높네요. 또 **깎아 줍니다.** $f$가 `단언컨대 이 영화 재미 있어요`라는 입력을 받았을 때 긍정 점수는 높아지고, 중립/부정 점수는 낮아지도록 모델 전체를 업데이트해 줍니다.

## **그림3** 감성 분석 모델의 출력 (3)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/e87KFzd.jpg" width="250px" title="source: imgur.com" />

이런 업데이트를 여러번 수행하면 종국에는 $f$가 아래 그림처럼 정답에 가까운 출력을 낼 수 있습니다. 이렇게 모델을 업데이트하는 과정 전체를 학습(train)이라고 부릅니다. 모델이 입력-출력 사이의 패턴을 스스로 익히는 과정입니다.

## **그림4** 감성 분석 모델의 출력 (4)
{: .no_toc .text-delta }
<img src="https://i.imgur.com/5Dzq7Qz.jpg" width="250px" title="source: imgur.com" />


---