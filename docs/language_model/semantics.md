---
layout: default
title: Pretrained LM
parent: Language Model
nav_order: 1
---

# Pretrained LM
{: .no_toc }

최근 BERT, GPT 같은 모델이 주목을 받게 된 이유는 성능 때문입니다. 이 모델들을 사용하면 문서 분류, 개체명 인식 등 어떤 태스크든 점수가 이전 대비 큰 폭으로 오르기 때문인데요. BERT, GPT 따위의 부류는 **미리 학습된 언어 모델(pretrained language model)**이라는 공통점이 있습니다. 이 장에서는 언어모델 개념과 프리트레인의 종류, 왜 미리 학습된 언어모델이 잘 되는지 이유 등을 살펴보고자 합니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---

## 언어 모델

**언어모델(Language Model)이란 단어 시퀀스에 확률을 부여하는 모델**입니다. 다시 말해 단어 시퀀스를 입력으로 하고, 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력으로 하는 모델입니다. 따라서 한국어 말뭉치로 학습한 언어모델은 자연스러운 한국어 문장에 높은 확률값을 부여합니다. 어떤 문장이 한국어스러운지 해당 모델이 이해하고 있다는 것이지요.

문장에서 $i$번째로 등장하는 단어를 $w_i$로 표시한다면 $n$개 단어로 구성된 문장이 해당 언어에서 등장할 확률, 즉 언어모델의 출력은 수식1과 같이 쓸 수 있습니다. 수식1의 구체적 의미는 $n$개 단어가 동시에 나타날 **결합확률(joint probability)**입니다. 잘 학습된 한국어 언어모델이 있다면 $P(무모, 운전)$보다는 $P(난폭, 운전)$이 큰 확률값을 지닐 겁니다.

## **수식1** 언어모델 (1)
{: .no_toc .text-delta }

$$
P(w_1, w_2, w_3, w_4, w_5, ... ,w_n)
$$

수식1에서처럼 `난폭`이라는 단어와 `운전`이라는 단어가 동시에 등장할 결합확률은 $P(난폭, 운전)$으로 표기합니다. 그렇다면 `난폭`이라는 단어가 나타난 다음에 `운전`이라는 단어가 나타날 확률은 어떻게 정의할까요? 이러한 확률을 **조건부 확률(conditional probability)**이라고 하는데요. 수식2와 같습니다.

## **수식2** 조건부 확률
{: .no_toc .text-delta }

$$
P(운전 | 난폭)=\frac { P(난폭, 운전) }{ P(난폭) } 
$$

수식2에서 확인할 수 있듯 조건부 확률을 표기할 때 조건이 되는 사건(`난폭`)은 뒤에, 결과가 되는 사건(`운전`)은 앞에 씁니다. 조건이 되는 사건이 우변 분자를 구성하고 있음을 볼 수 있는데요. 결과가 되는 사건(`운전`)은 조건이 되는 사건(`난폭`)의 영향을 받아 변한다는 개념을 내포하고 있습니다. 그도 그럴 것이 앞선 단어가 `난폭`이라면 다음 단어로 어떤 것이 자연스러울지 그 선택지가 확 줄어들겠지요.

결합확률과 조건부 확률 사이에는 밀접한 관련이 있습니다. 예컨대 $P(w_1,w_2,w_3)=P(w_1)\times P(w_2\|w_1)\times P(w_3\|w_1,w_2)$입니다. 수식2의 조건부 확률의 정의에 따라 우변을 쭉 펼쳐 계산해보면 좌변이 성립함을 알 수 있습니다. 이를 직관적으로 곱씹어보면, 단어 3개로 구성된 문장이 나타나려면(=단어 3개가 동시에 등장하려면) 아래 3개 사건이 동시에 일어나야 한다는 말이 됩니다.

- 첫번째 단어가 등장
- 첫번째 단어가 등장한 후 두번째 단어가 등장
- 첫번째 단어와 두번째 단어가 등장한 후 세번째 단어가 등장

이로부터 수식1의 언어모델을 조건부 확률 개념으로 다시 쓸 수 있습니다. 수식3과 같습니다. 요컨대 **전체 단어 시퀀스가 나타날 확률**은 **이전 단어들이 주어졌을 때 다음 단어가 등장할 확률의 연쇄**와 동치라는 이야기입니다.


## **수식3** 언어모델 (2)
{: .no_toc .text-delta }

$$
P(w_1, w_2, w_3, w_4, w_5, ... w_n) = \prod_{i=1}^{n}P(w_{n} | w_{1}, ... , w_{n-1})
$$


### 순방향 언어모델

임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지 이해하고 있는 언어모델을 구축하려고 합니다. 그런데 이미 우리는 수식3의 좌변과 우변이 동치임을 확인했습니다. 이 때문에 언어모델의 계산 로직을 **컨텍스트(context)가 주어졌을 때 다음 단어 맞추기**로 상정해 두어도 원래 목표를 달성할 수 있습니다.

그림1은 학습 말뭉치가 `어제 카페 갔었어 거기 사람 많더라`라는 문장 하나일 때 언어모델이 계산하는 대상을 도식적으로 나타낸 것입니다. 이같이 문장 처음부터 끝으로 사람이 이해하는 순서대로 계산하는 모델을 **순방향(forward) 언어모델**이라고 합니다. GPT, ELMo 같은 모델이 이같은 방식으로 프리트레인을 수행합니다.

## **그림1** 순방향(left-to-right) 언어모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/4dv6TNZ.png" width="400px" title="source: imgur.com" />


### 역방향 언어모델

그림1은 그림2는 같은 데이터지만 문장 끝에서 처음으로 계산하는 **역방향(backward) 언어모델**을 나타내고 있습니다. 순방향 언어모델처럼 역방향 언어모델 역시 방향만 바뀌었을 뿐 다음 단어 맞추기 과정에서 전체 단어 시퀀스가 나타날 확률을 계산할 수 있습니다. ELMo 같은 모델이 이같은 방식으로 프리트레인을 수행합니다.

## **그림2** 역방향(right-to-left) 언어모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/VHB5dsR.png" width="400px" title="source: imgur.com" />

---

## 넓은 의미의 언어모델

전통적인 의미의 언어모델은 수식3이었습니다. 하지만 최근에는 언어모델을 수식4처럼 정의하기도 합니다. 수식4는 주변 맥락 정보가 전제된 상태에서 특정 단어가 나타날 조건부 확률을 나타내고 있습니다. 수식4처럼 정의된 언어모델은 컨텍스트(단어 혹은 단어 시퀀스)를 입력 받아 특정 단어가 나타날 확률을 출력으로 반환합니다. 수식4의 컨텍스트와 맞출 단어를 어떻게 설정하느냐에 따라 다양한 변형이 존재합니다.

## **수식4** 언어모델 (2)
{: .no_toc .text-delta }

$$
P(w | \text{context} )
$$


### 마스크 언어모델

마스크 언어모델(Maksed Language Model)은 학습 대상 문장에 빈칸을 만들어 놓고 해당 빈칸에 올 단어로 적절한 단어가 무엇일지 분류하는 과정에서 학습합니다. BER가 마스크 언어모델로 프리트레인되는 대표적인 모델입니다. 그림3은 마스크 언어모델을 도식적으로 그린 그림입니다.

그림3의 첫번째 줄에서 컨텍스트는 `[MASK] 카페 갔었어 거기 사람 많더라`가 됩니다. 맞출 대상이 되는 단어는 `어제`가 됩니다. 마찬가지로 두번째 줄의 컨텍스트는 `어제 [MASK] 갔었어 거기 사람 많더라`가 되며 맞출 단어는 `카페`가 됩니다. 

## **그림3** 마스크 언어모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/YJjh69r.png" width="400px" title="source: imgur.com" />

그림3에서 확인할 수 있듯 맞출 단어 이전 단어들만 참고할 수 있는 순방향/역방향 언어모델과 달리 마스크 언어모델은 맞출 단어 계산시 문장 전체의 맥락을 참고할 수 있다는 장점이 있습니다. 이 때문에 마스크 언어모델에 양방향(bidirectional) 성질이 있다고들 합니다. 다시 말해 맞출 단어 앞뒤를 모두 본다는 뜻입니다.


### 스킵-그램 모델

스킵-그램 모델(Skip-Gram Model)은 어떤 단어 앞뒤에 특정 범위(윈도우)를 설정해 두고 이 범위 내에 어떤 단어들이 올지 분류하는 과정에서 학습합니다. 그림5에서 수식4의 컨텍스트가 되는 단어는 `거기`입니다. 윈도우 크기는 2라고 설정해 둔 상황입니다. 컨텍스트 단어 앞뒤로 두 개씩 본다는 뜻입니다. 

이 경우 스킵-그램 모델은 `거기` 주변에 `카페`가 나타날 확률, `거기` 주변에 `갔었어`가 나타날 확률, `거기` 주변에 `사람`이 나타날 확률, `거기` 주변에 `많더라`가 나타날 확률을 각각 높이는 방식으로 학습합니다. 요컨대 스킵-그램 모델은 `거기`라는 단어 주변(단 윈도우 범위 내)에 어떤 단어들이 분포해 있는지를 학습한다는 이야기입니다.

## **그림5** 스킵-그램 모델
{: .no_toc .text-delta }
<img src="https://i.imgur.com/hj85TZx.png" width="400px" title="source: imgur.com" />

2012년 구글에서 발표한 단어 수준 임베딩 기법인 Word2Vec이 스킵-그램 모델 방식으로 학습합니다.

---

## 언어 모델의 유용성

잘 학습된 언어모델은 그 자체로 값어치가 있습니다. 어떤 문장이 자연스러운지 가려낼 수 있기 때문입니다. 이 때문에 기계 번역, 문법 교정, 문장 생성 등 다양한 태스크를 수행할 수 있습니다.

- 기계 번역 : $P(\text{죽음을 피할 수 없다})$ > $P(\text{죽음으로부터 자유로울 수 없다})$
- 문법 교정 : $P(\text{두시 삼십 이분})$ > $P(\text{이시 서른 두분})$
- 문장 생성 : $P(\text{?}\|{발 없는 말이})$

학습 대상 언어의 풍부한 맥락이 내재화돼 있는 점 역시 큰 장점입니다. 이와 관련해 그림4는 이준범 님이 공개한 [kcbert 모델의 계산 결과](https://huggingface.co/beomi/kcbert-large)를 나타냅니다. 이 모델은 12기가바이트(GB) 크기의 네이버 댓글 데이터로 학습한 BERT인데요. 마스크 언어모델 방식으로 프리트레인됐습니다.

## **그림4** kcbert
{: .no_toc .text-delta }
<img src="https://i.imgur.com/nv9A8i2.png" width="400px" title="source: imgur.com" />

이 모델에 `어제 [MASK] 갔었어 거기 사람 많더라`를 입력하니 해당 마스크 위치에 마침표(확률값 0.131)가 오는 것이 가장 자연스럽다고 예측하고 있군요. 물음표(0.119), 생각보다(0.106), 물결(0.092), 어쩐지(0.054) 등이 그 뒤를 잇고 있습니다. 모두 그럴듯한 한국어 문장임을 확인할 수 있군요.

그림5는 OpenAI가 2020년 공개한 초거대규모 언어모델 GPT3의 단순 계산 능력을 평가한 그래프입니다. '다음 단어 맞추기'라는 단순 태스크로만 프리트레인을 했음에도 가장 큰 모델인 `175B`는 두 자릿수 덧셈/뺄셈에서 거의 100%에 가까운 정확도를 나타내고 있습니다. 해당 모델이 학습 말뭉치를 그대로 외운 것 같다는 인상을 주지만, 큰 언어모델에 학습 대상 언어의 풍부한 맥락이 내재화되어 있다는 점은 의심할 여지가 없는 것 같습니다.

## **그림5** GPT3의 단순 계산 능력
{: .no_toc .text-delta }
<img src="https://i.imgur.com/FjrJwCS.png" width="400px" title="source: imgur.com" />

최근 들어 언어모델이 주목받고 있는 이유 가운데 하나는 데이터 제작 비용입니다. '다음 단어 맞추기' 혹은 '빈칸 맞추기' 등으로 학습 태스크를 구성하면 사람이 일일이 수작업해야 하는 레이블 없이도 다량의 학습데이터를 아주 싼 값에 만들어낼 수 있습니다. GPT3 같은 초거대규모 언어모델이 탄생하게 된 배경이기도 하죠.

또다른 이유는 [트랜스퍼 러닝(Transfer Learning)](https://ratsgo.github.io/nlpbook/docs/introduction/transfer)을 꼽을 수 있을 것 같습니다. 대량의 말뭉치로 프리트레인한 언어모델을 문서 분류, 개체명 인식 등 다운스트림 태스크에 적용하면 적은 양의 다운스트림 데이터로도 그 성능을 큰폭으로 올릴 수 있습니다.

실제로 자연어 처리 분야에서 최근 제안되는 기법들은 프리트레인을 마친 딥러닝 계열 언어모델을 바탕으로 하는 경우가 대다수입니다. 이 언어모델의 최종 혹은 중간 출력값을 가지고 다양한 태스크를 수행합니다. 이러한 출력값을 **임베딩(embedding)** 혹은 **리프레젠테이션(representation)**이라고 부릅니다. 


---

## 참고문헌

- [한국어 임베딩](http://www.yes24.com/Product/Goods/78569687)
- [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)
- [Language Modeling](http://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)
- [Language Model - Wikipedia](https://en.m.wikipedia.org/wiki/Language_model)

---

