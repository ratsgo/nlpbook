---
layout: default
title: Language Model
nav_order: 4
has_children: true
has_toc: false
permalink: /docs/lm
---

# 숫자 세계로 떠난 자연어
{: .no_toc }

최근의 자연어 처리는 다량의 말뭉치의 의미, 문맥을 학습한 언어모델(language model)을 활용해 문서 분류, 개체명 인식 등 각종 태스크를 수행합니다. 이 언어모델은 세부 태스크의 성능을 좌우하는데요. 요즘 들어서는 트랜스포머(transformer) 기반의 언어모델이 각광받고 있습니다. 이 장에서는 ∆ 언어모델이 말뭉치의 어떤 의미 정보를 학습하는지 ∆ 트랜스포머의 핵심 동작 원리는 무엇인지 ∆ 트랜스포머가 기본 뼈대인 BERT, GPT 모델의 특징 등을 살펴보겠습니다.

이 장은 이전 장보다 꽤 어렵습니다. 당장 소화하기 어렵다면 다음 장으로 건너뛰어도 실습을 수행하고 전체 맥락을 이해하는 데 큰 문제가 없습니다. 하지만 도전 가치는 충분합니다. 차근차근 따라가 보시죠!

---
