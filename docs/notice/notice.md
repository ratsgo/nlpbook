---
layout: default
title: Paper Book Notice
nav_order: 10
has_children: false
has_toc: false
permalink: /docs/notice
---

# 종이책 정오표
{: .no_toc }

종이책 오탈자 및 수정 사항을 안내합니다. 해당 내용은 [nlpbook issue](https://github.com/ratsgo/nlpbook/issues)에서 토론을 거쳐 확정된 것입니다. 여기에 나오지 않는 사항이라도 언제든지 편하게 리포트 해주세요! 정정 의견은 [github.com](https://github.com)에 회원 가입을 한 뒤 [nlpbook issue 등록 항목](https://github.com/ratsgo/nlpbook/issues/new)에서 제목과 본문을 입력하면 등록할 수 있습니다.
{: .fs-4 .ls-1 .code-example }

## Table of contents
{: .no_toc .text-delta .mt-6}

1. TOC
{:toc}

---

## 26페이지

코드 1-1을 다음으로 교체.

```python
from ratsnlp.nlpbook.classification import ClassificationTrainArguments 
args = ClassificationTrainArguments(
	pretrained_model_name="beomi/kcbert-base", 
	downstream_corpus_name="nsmc", 
	downstream_corpus_root_dir="/content/Korpora", 
	downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-doccls", 
	learning_rate=5e-5,
	batch_size=32, 
)
```

다음 문장을 교체.

**수정 전**

>  즉, 코드 1-1에서 설정한 args에 따라 nsmc를 /root/Korpora 디렉터리에 저장합니다.

**수정 후**

>  즉, 코드 1-1에서 설정한 args에 따라 nsmc를 코랩 환경 로컬의 /content/Korpora 디렉터리에 저장합니다.

---

## 54페이지

다음 문장을 교체.

**수정 전**

> 다음 코드를 수행하면 NSMC에 포함된 영화 리뷰들을 순수 텍스트 형태로 지정된 디렉터리에 저장해 둡니다.

**수정 후**

>  다음 코드를 수행하면 NSMC에 포함된 영화 리뷰들을 순수 텍스트 형태로 코랩 환경 로컬의 지정된 디렉터리에 저장해 둡니다.


코드 2-4를 다음으로 교체.


```python
import os
def write_lines(path, lines):
    with open(path, 'w', encoding='utf-8') as f: 
    	for line in lines:
    		f.write(f'{line}\n') 
write_lines("/content/train.txt", nsmc.train.get_all_texts())
write_lines("/content/test.txt", nsmc.test.get_all_texts())
```


다음 문단을 교체.

**수정 전**

> 한글은 한 글자가 3개의 유니코드 바이트로 표현되는데요, 예를 들어 안녕하세요라는 문자열을 유니코드 바이트로 변환하면 다음처럼 됩니다. 안녕하세요 → ìķĪëħķíķĺìĦ ̧ìļĶ

> 바이트 수준으로 BPE를 수행한다는 것은 어휘 집합 구축 대상 말뭉치를 위와 같이 모두 유니 코드 바이트로 변환하고 이들을 문자 취급해 가장 자주 등장한 문자열을 병합하는 방식으로 어휘 집합을 만든다는 의미입니다. 토큰화 역시 문자열을 유니코드 바이트로 일단 변환한 뒤 수행합니다.

**수정 후**

> 한글은 한 글자가 3개의 유니코드 바이트로 표현되는데요, 예를 들어 안녕하세요라는 문자열을 각각의 유니코드 바이트에 대응하는 문자\*로 변환하면 다음처럼 됩니다. 안녕하세요 → ìķĪëħķíķĺìĦ ̧ìļĶ

> \* 유니코드(UTF-8) 1바이트를 10진수로 표현하면 0에서 255 사이의 정수가 됩니다. 이 256개 정수 각각을 특정 문자로 매핑한 것입니다. 예컨대 0은 Ā, 255는 ÿ에 각각 대응합니다.

> 바이트 수준으로 BPE를 수행한다는 것은 어휘 집합 구축 대상 말뭉치를 위와 같이 변환하고 이들을 문자 취급해 가장 자주 등장한 문자열을 병합하는 방식으로 어휘 집합을 만든다는 의미입니다. 토큰화 역시 원래 문자열을 위와 같이 변환한 뒤 수행합니다.


---

## 55페이지

코드 2-6을 다음으로 교체.

```python
from tokenizers import ByteLevelBPETokenizer
bytebpe_tokenizer = ByteLevelBPETokenizer()
bytebpe_tokenizer.train(
    files=["/content/train.txt", "/content/train.txt"],
    vocab_size=10000, 
    special_tokens=["[PAD]"]
)
bytebpe_tokenizer.save_model("/gdrive/My Drive/nlpbook/bbpe")
```

---

## 56페이지

코드 2-8을 다음으로 교체.

```python
from tokenizers import BertWordPieceTokenizer
wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)
wordpiece_tokenizer.train(
    files=["/content/train.txt", "/content/train.txt"],
    vocab_size=10000,
)
wordpiece_tokenizer.save_model("/gdrive/My Drive/nlpbook/wordpiece")
```

---

## 96페이지

다음 문단을 교체.

**수정 전**

> 다음 그림은 쿼리가 cafe일 때 셀프 어텐션을 나타낸 것입니다. 학습이 잘 되었다면 쿼리, 키로부터 계산한 소프트맥스 확률 가운데 장소를 지칭하는 대명사 There가 높은 값을 지닐 겁니다. 이 확률과 밸류 벡터를 가중합해서 셀프 어텐션 계산을 마칩니다.

**수정 후**

> 그림 3-32는 쿼리가 cafe일 때 마스크 멀티 헤드 어텐션을 나타낸 것입니다. 학습이 잘 되었다면 쿼리, 키로부터 계산한 소프트맥스 확률 가운데 장소를 지칭하는 대명사 There가 높은 값을 지닐 겁니다. 이 확률과 밸류 벡터를 가중합해서 계산을 마칩니다.

다음 그림 설명을 교체.

**수정 전**

> 그림 3-32 타깃 문장의 셀프 어텐션

**수정 후**

> 그림 3-32 마스크 멀티 헤드 어텐션


---

## 97페이지

다음 문단을 교체.

**수정 전**

> 다음 그림은 쿼리 단어가 cafe인 셀프 어텐션 계산을 나타낸 것입니다. 학습이 잘 되었다면 쿼리(타깃 언어 문장), 키(소스 언어 문장)로부터 계산한 소프트맥스 확률 가운데 쿼리에 대응하는 해당 장소를 지칭하는 단어 카페가 높은 값을 지닐 겁니다. 이 확률과 밸류 벡터를 가중합해서 셀프 어텐션 계산을 마칩니다.

**수정 후**

> 다음 그림은 쿼리 단어가 cafe인 멀티 헤드 어텐션 계산을 나타낸 것입니다. 학습이 잘 되었다면 쿼리(타깃 언어 문장), 키(소스 언어 문장)로부터 계산한 소프트맥스 확률 가운데 쿼리에 대응하는 해당 장소를 지칭하는 단어 카페가 높은 값을 지닐 겁니다. 이 확률과 밸류 벡터를 가중합해서 계산을 마칩니다.

다음 문단을 교체.

**수정 전**

> 따라서 정답을 포함한 타깃 시퀀스의 미래 정보를 셀프 어텐션 계산에서 제외(마스킹)하게 됩니다. 이 때문에 디코더 블록의 첫 번째 어텐션을 마스크 멀티 헤드 어텐션이라고 부릅니 다. 다음 그림과 같습니다. 마스킹은 소프트맥스 확률이 0이 되도록 하여, 밸류와의 가중합에 서 해당 단어 정보들이 무시되게끔 하는 방식으로 수행됩니다.

**수정 후**

> 따라서 정답을 포함한 타깃 시퀀스의 미래 정보를 셀프 어텐션 계산에서 제외(마스킹)하게 됩니다. 다음 그림과 같습니다. 구체적으로는 타깃 시퀀스에 대한 마스크 멀티 헤드 어텐션 계산시 제외 대상 단어들의 소프트맥스 확률이 0이 되도록 하여 멀티 헤드 어텐션에서도 해당 단어 정보들이 무시되게끔 하는 방식으로 수행됩니다.


다음 그림 설명을 교체.

**수정 전**

> 그림 3-33 소스-타깃 문장 간 셀프 어텐션

**수정 후**

> 그림 3-33 디코더에서 수행되는 멀티 헤드 어텐션


---

## 98페이지

다음 문장을 교체.

**수정 전**

> 이전 그림처럼 셀프 어텐션을 수행하면 디코더 마지막 블록 출력 벡터 가운데 \<s\>에 해당하는 벡터에는 소스 문장 전체의 문맥적 관계성이 함축되어 있습니다. 

**수정 후**

> 이전 그림처럼 셀프 어텐션을 수행하면 디코더 마지막 블록 출력 벡터 가운데 \<s\>에 해당하는 벡터에는 소스 문장 전체(어제 ... 많더라)와 \<s\> 사이의 문맥적 관계성이 함축되어 있습니다. 

다음 문장을 교체.

**수정 전**

> 따라서 이때의 셀프 어텐션은 정답 단어 I 이후의 모든 타깃 언어 단어들을 모델이 보지 못하도록 하는 방식으로 수행됩니다.

**수정 후**

> 따라서 이때의 디코더 쪽 셀프 어텐션은 정답 단어 went 이후의 모든 타깃 언어 단어들을 모델이 보지 못하도록 하는 방식으로 수행됩니다.

---

## 99페이지

다음 문장을 교체.

**수정 전**

> 디코더 마지막 블록의 I 벡터에는 소스 문장(어제 ... 갔더라)과 \<s\> I 사이의 문맥적 관계 성이 녹아 있습니다. 

**수정 후**

> 디코더 마지막 블록의 I 벡터에는 소스 문장(어제 ... 많더라)과 \<s\> I 사이의 문맥적 관계 성이 녹아 있습니다. 

다음 문장을 교체.

**수정 전**

> 따라서 이때의 셀프 어텐션은 정답 단어 to 이후의 모든 타깃 언어 단어들을 모델이 보지 못하도록 하는 방식으로 수행됩니다. 

**수정 후**

> 따라서 이때의 디코더 쪽 셀프 어텐션은 정답 단어 to 이후의 모든 타깃 언어 단어들을 모델이 보지 못하도록 하는 방식으로 수행됩니다.

---

## 104페이지

다음 문장을 교체.

**수정 전**

> 다음 그림과 은닉층 뉴런값이 같고 그에 대한 가중치가 \[1 2 1\]이라면

**수정 후**

> 은닉층 뉴런값이 이전 그림과 같고 그에 대한 가중치가 \[1 2 1\]이라면

---

## 106페이지

다음 문장을 교체.

**수정 전**

> h_preact와 h는 그림 3-48\~50에 이르는 은닉층 손 계산 예시와 똑같은 결과임을 알 수 있습니다. y는 그림 3-51과 3-52에 해당하는 출력층 손 계산 예시와 같은 결과입니다.

**수정 후**

> h_preact와 h는 그림 3-44\~46 이르는 은닉층 손 계산 예시와 똑같은 결과임을 알 수 있습니다. y는 그림 3-47과 3-48에 해당하는 출력층 손 계산 예시와 같은 결과입니다.


---

## 135페이지

다음 문장을 교체.

**수정 전**

> downstream_corpus_root_dir: 다운스트림 데이터를 내려받을 위치. 입력하지 않으면 /root/Korpora 에 저장됩니다.

**수정 후**

> downstream_corpus_root_dir: 다운스트림 데이터를 내려받을 위치. 입력하지 않으면 코랩 환경 로컬의 /content/Korpora 에 저장됩니다.


---

## 136페이지

다음 문장을 교체.

**수정 전**

> 데이터를 내려받는 도구로 코포라 Korpora라는 파이썬 오픈소스 패키지를 사용해, corpus_name(nsmc)에 해당하는 말뭉치를 root_dir(/root/Korpora) 아래에 저장해 둡니다.

**수정 후**

> 데이터를 내려받는 도구로 코포라 Korpora라는 파이썬 오픈소스 패키지를 사용해, corpus_name(nsmc)에 해당하는 말뭉치를 코랩 환경 로컬의 root_dir(/content/Korpora) 아래에 저장해 둡니다.

---


## 139페이지

다음 문장을 교체.

**수정 전**

> 토큰 기준 최대 길이(max_seq_length)를 코드 4-3의 args에서 128로 설정해 두었기 때문입니다.

**수정 후**

> 토큰 기준 최대 길이(max_seq_length)를 코드 4-4의 args에서 128로 설정해 두었기 때문입니다.


---


## 149페이지

다음 문장을 교체.

**수정 전**

> 마지막으로 모델 출력을 약간 후처리하여 예측 확률의 최댓값이 부정 위치일 때 해당 문장이 ‘부정 (positive)’, 반대일 때는 ‘긍정 (positive)’이 되도록 pred값을 만듭니다.

**수정 후**

> 마지막으로 모델 출력을 약간 후처리하여 예측 확률의 최댓값이 부정 위치일 때 해당 문장이 ‘부정 (negative)’, 반대일 때는 ‘긍정 (positive)’이 되도록 pred값을 만듭니다.


---

## 160페이지

다음 문장을 교체.

**수정 전**

> 다음 코드를 실행하면 KLUE-NLI 데이터를 내려받습니다. corpus_name에 해당하는 말뭉치(klue-nli)를 downstream_corpus_root_dir 아래(/root/Korpora)에 저장해 둡니다.

**수정 후**

> 다음 코드를 실행하면 KLUE-NLI 데이터를 내려받습니다. corpus_name에 해당하는 말뭉치(klue-nli)를 코랩 환경 로컬의 downstream_corpus_root_dir 아래(/content/Korpora)에 저장해 둡니다.

---

## 178페이지

다음 문장을 교체.

**수정 전**

> 다음 코드를 실행하면 corpus_name(ner)에 해당하는 말뭉치를 내려받습니다.

**수정 후**

> 다음 코드를 실행하면 corpus_name(ner)에 해당하는 말뭉치를 코랩 환경 로컬에 내려받습니다.


---

## 221페이지

다음 문장을 교체.

**수정 전**

> 다음 코드를 실행하면 NSMC 말뭉치를 내려받습니다. 데이터를 내려받는 도구로 오픈소스
패키지 Korpora를 사용해 corpus_name에 해당하는 말뭉치(nsmc)를 root_dir(/root/Korpora) 아래에 저장해 둡니다.

**수정 후**

> 다음 코드를 실행하면 NSMC 말뭉치를 내려받습니다. 데이터를 내려받는 도구로 오픈소스
패키지 Korpora를 사용해 corpus_name에 해당하는 말뭉치(nsmc)를 코랩 환경 로컬의 root_dir(/content/Korpora) 아래에 저장해 둡니다.

---

## 237페이지

다음 문단을 교체.

**수정 전**

> 샘플링 방식 예를 든 다음 그림을 보면 그라는 컨텍스트를 입력했을 때 모델은 다음 토큰으로 집(0.5), 책(0.4), 사람(0.1)이 그럴듯하다고 예측했습니다. 여기에서 다음 토큰을 확률적으로 선택합니다. 집이 선택될 가능성이 50%로 제일 크고 사람이 선택될 가능성도 10%로 작지만 없지 않습니다.

**수정 후**

> 샘플링 방식 예를 든 다음 그림을 보면 그라는 컨텍스트를 입력했을 때 모델은 다음 토큰으로 책(0.5), 집(0.4), 사람(0.1)이 그럴듯하다고 예측했습니다. 여기에서 다음 토큰을 확률적으로 선택합니다. 책이 선택될 가능성이 50%로 제일 크고 사람이 선택될 가능성도 10%로 작지만 없지 않습니다.


그림 8-8을 다음으로 교체.

<img width="500" src="https://user-images.githubusercontent.com/26211652/147377581-e8abb8d9-6db6-4a0b-aa37-d7e6497db2a7.png">

---

## 240페이지


그림 8-11을 다음으로 교체.

<img width="500" src="https://user-images.githubusercontent.com/26211652/147377570-42915486-d087-49f3-8834-2d579654d5ee.png">


---