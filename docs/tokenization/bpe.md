---
layout: default
title: ↗️ Byte Pair Encoding
parent: Preprocess
nav_order: 2
---

# ↗️ Byte Pair Encoding
{: .no_toc }

바이트 페어 인코딩(Byte Pair Encoding, BPE)은 원래 정보 압축을 위해 제안된 알고리즘으로 최근 자연어 처리 모델에 널리 쓰이고 있는 토큰화 기법입니다. GPT 모델은 BPE 기법으로 토큰화를 수행하며, BERT 모델은 토크나이저로 BPE의 변형된 버전인 워드피스(wordpiece)를 사용하고 있습니다. 이 장에서는 BPE 기법 전반을 살펴봅니다.
{: .fs-4 .ls-1 .code-example }

1. TOC
{:toc}


---

## 바이트 페어 인코딩

바이트 페어 인코딩(Byte Pair Encoding, BPE)는 1994년 제안된 정보 압축 알고리즘으로 데이터에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 기법입니다. 예컨대 우리가 가진 데이터가 다음과 같다고 해 봅시다.

- aaabdaaabac

이 문자열에선 `aa`가 가장 많이 나타났으므로 이를 `Z`로 병합(치환)하면 위의 문자열을 다음과 같이 압축할 수 있습니다.

- ZabdZabac

이 문자열은 한번 더 압축 가능합니다. 살펴보니 `ab`가 가장 많이 나타났으므로 이를 `Y`로 병합(치환)합니다. 다음과 같습니다.

- ZYdZYac

BPE 수행 이전에는 원래 데이터를 표현하기 위한 사전 크기가 3개(`a`, `b`, `c`)였습니다. 수행 이후엔 그 크기가 5개(`a`, `b`, `c`, `Z`, `Y`)로 늘었습니다. 반면 데이터의 길이는 11개에서 7로 줄었습니다. 이처럼 BPE는 사전 크기를 지나치게 늘리지 않으면서도 각 데이터를 효율적으로 압축할 수 있도록 합니다.

BPE 기반 토큰화 기법은 분석 대상 언어에 대한 지식이 필요 없습니다. 말뭉치에서 자주 나타나는 문자열(서브워드)을 토큰으로 분석하기 때문입니다. 실제로 자연어 처리에서 BPE가 처음 쓰인 것은 기계 번역 분야입니다. BPE를 활용한 토크나이즈 절차는 다음과 같습니다.

1. **어휘 집합 구축** : 자주 등장하는 문자열(위의 예시에서 `aa`, `ab`들)를 병합하고 이를 어휘 집합(사전)에 추가한다. 이를 원하는 어휘 집합 크기가 될 때까지 반복한다.
2. **토큰화** : 토큰화 대상 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에서 어휘 집합에 있는 서브워드가 포함되어 있을 경우 해당 서브워드를 어절에서 분리한다.

각 단계를 이후 장에서 차례대로 살펴보겠습니다.

---

## BPE 어휘 집합 구축

BPE 어휘 집합 구축 절차를 구체적으로 살펴보겠습니다. 어휘 집합을 만드려면 우선 말뭉치를 준비해야 합니다. 말뭉치의 모든 문장을 공백으로 나눠줍니다. 이를 프리토크나이즈(pre-tokenize)라고 합니다. 본격적인 토큰화에 앞서 미리 분석했다는 의미에서 이런 이름이 붙은 것 같습니다. 물론 공백 말고 다른 기준으로 프리토크나이즈를 수행할 수도 있습니다.

우리가 가진 말뭉치에 프리토크나이즈를 실시하고 그 빈도를 모두 세어서 표1을 얻었다고 가정해 봅시다.

## **표1** 프리토크나이즈 결과
{: .no_toc .text-delta }  

|토큰|빈도|
|---|---|
|hug|10|
|pug|5|
|pun|12|
|bun|4|
|hugs|5|

BPE를 문자(character) 단위로 수행할 경우 초기의 어휘 집합은 다음과 같습니다. 

- `b`, `g`, `h`, `n`, `p`, `s`, `u`

이들로도 표1의 모든 토큰을 표현할 수 있습니다. 하지만 우리는 사전 크기가 약간 증가하더라도 토큰 시퀀스 길이를 줄이려는(정보를 압축하려는) 목적을 가지고 있으므로 BPE를 수행해 줄 계획입니다. 초기 어휘 집합을 바탕으로 표1을 다시 쓰면 표2와 같습니다.

## **표2** BPE 어휘 집합 구축 (1)
{: .no_toc .text-delta } 

|토큰|빈도|
|---|---|
|h, u, g|10|
|p, u, g|5|
|p, u, n|12|
|b, u, n|4|
|h, u, g, s|5|

표3은 표2의 토큰을 두 개(바이그램, bigram)씩 묶어 쭉 나열한 것입니다. 표4는 바이그램 쌍을 키값으로 해서 표3의 빈도를 합쳐준 결과입니다.

## **표3** BPE 어휘 집합 구축 (2)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|h, u|10|
|u, g|10|
|p, u|5|
|u, g|5|
|p, u|12|
|u, n|12|
|b, u|4|
|u, n|4|
|h, u|5|
|u, g|5|
|g, s|5|

## **표4** BPE 어휘 집합 구축 (3)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|b, u|4|
|g, s|5|
|h, u|15|
|p, u|17|
|u, g|20|
|u, n|16|

가장 많이 등장한 바이그램 쌍은 `u, g`로 총 20회입니다. 따라서 `u`와 `g`를 합친 `ug`를 어휘 집합에 추가합니다. 다음과 같습니다.

- `b`, `g`, `h`, `n`, `p`, `s`, `u`, `ug`

표5는 표2를 새로운 어휘 집합에 맞게 다시 쓴 결과입니다. `u`와 `g`를 병합했기 때문에 표2의 각 빈도는 그대로인 채 `h, u, g`가 `h, ug`, `p, u, g`가 `p, ug`, `h, u, g, s`가 `h, ug, s`로 바뀌었음을 확인할 수 있습니다.

## **표5** BPE 어휘 집합 구축 (4)
{: .no_toc .text-delta } 

|토큰|빈도|
|---|---|
|h, ug|10|
|p, ug|5|
|p, u, n|12|
|b, u, n|4|
|h, ug, s|5|

표6은 표5를 바이그램 쌍 빈도를 나타낸 결과입니다. 계산 과정은 표3, 표4와 동일합니다.

## **표6** BPE 어휘 집합 구축 (5)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|b, u|4|
|h, ug|15|
|p, u|12|
|p, ug|5|
|u, n|16|
|ug, s|5|

이번에 가장 많이 등장한 바이그램 쌍은 `u, n`으로 총 16회입니다. 따라서 `u`와 `n`을 합친 `un`을 어휘 집합에 추가합니다. 다음과 같습니다.

- `b`, `g`, `h`, `n`, `p`, `s`, `u`, `ug`, `un`

표7은 표2를 새로운 어휘 집합에 맞게 다시 쓴 결과, 표8은 표7을 바탕으로 바이그램 쌍 빈도를 나타낸 결과입니다.

## **표7** BPE 어휘 집합 구축 (6)
{: .no_toc .text-delta } 

|토큰|빈도|
|---|---|
|h, ug|10|
|p, ug|5|
|p, un|12|
|b, un|4|
|h, ug, s|5|

## **표8** BPE 어휘 집합 구축 (7)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|b, un|4|
|h, ug|15|
|p, ug|5|
|p, un|12|
|ug, s|5|

이번에 가장 많이 등장한 바이그램 쌍은 `h`, `ug`로 총 15회입니다. 따라서 `h`와 `ug`를 합친 `hug`를 어휘 집합에 추가합니다. 다음과 같습니다.

- `b`, `g`, `h`, `n`, `p`, `s`, `u`, `ug`, `un`, `hug`

BPE 어휘 집합 구축은 어휘 집합이 사용자가 정한 크기가 될 때까지 반복 수행합니다. 만일 어휘 집합 크기를 10개로 정해놓았다면, 어휘가 10개가 되었으므로 여기에서 BPE 어휘 집합 구축 절차를 마칩니다. 위의 어휘 집합은 허깅페이스 토크나이저 패키지를 활용한 [튜토리얼](https://ratsgo.github.io/nlpbook/docs/language_model/tutorial)에서 `vocab.json` 형태로 저장됩니다.

BPE 어휘 집합 구축 결과물로 `merge.txt`라는 것도 있습니다. 이는 BPE 토큰화 과정에서 서브워드 병합 순서를 정하는 데 쓰이는데요. 다음과 같이 만듭니다.

우선 표1을 최종 어휘 집합을 기준으로 다시 씁니다. 표9와 같습니다.

## **표9** BPE 어휘 집합 구축 (8)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|b, un|4|
|hug|15|
|p, ug|5|
|p, un|12|
|ug, s|5|

표10은 지금까지 계산해놓은 바이그램 쌍 빈도 표들을 빈도를 기준으로 내림차순 정렬한 것입니다. 표10에서 빈도를 제외한 바이그램 쌍들이 `merge.txt` 형태로 저장됩니다.

## **표10** BPE 어휘 집합 구축 (9)
{: .no_toc .text-delta } 

|바이그램 쌍|빈도|
|---|---|
|u, g|20|
|p, u|17|
|u, n|16|
|h, u|15|
|...|...|
|b, u|4|
|...|...|

---

## BPE 토큰화

어휘 집합(`vocab.json`)과 바이그램 쌍 빈도 표(`merge.txt`)가 있으면 토큰화를 수행할 수 있습니다. 예컨대 `pug bug mug`라는 문장을 토큰화한다고 가정해 봅시다. 그러면 일단 이 문장에 프리토크나이즈를 수행해 공백 단위로 분절합니다. 다음과 같습니다.

- `pug bug mug` > `pug`, `bug`, `mug`

위와 같이 분리된 토큰들 각각에 대해 BPE 토큰화를 수행합니다. 우선 `pug`가 대상입니다. 우리는 BPE 기본 단위로 문자를 상정하고 있으니, `pug`를 문자 단위로 분리합니다.

- `pug` > `p`, `u`, `g`

이후 `merge.txt`를 참고해 병합 우선 순위를 부여합니다.

- `p`, `u` : `merge.txt`(표10)에서 2순위
- `u`, `g` : `merge.txt`(표10)에서 1순위

둘 중에 `u`와 `g`의 우선순위가 높으므로 이들을 먼저 합쳐 줍니다. 다음과 같습니다.

- `p`, `u`, `g` > `p`, `ug`

이후 `merge.txt`를 참고해 병합 우선 순위를 부여합니다.

- `p`, `ug` : 해당 바이그램 쌍이 `merge.txt`에 존재하지 않음

더 이상 병합 대상이 존재하지 않으므로 병합을 그만둡니다. 그 다음으로는 `p`, `ug`가 각각 어휘 집합에 있는지를 검사합니다. 둘 모두 있으므로 `pug`의 토큰화 최종 결과로 `p`, `ug`를 반환합니다.

이번엔 `bug` 차례입니다. 같은 방식으로 수행하면 토큰화 결과가 `b`, `ug`가 됩니다.

마지막으로 `mug` 차례입니다. `merge.txt`를 참고해 병합 우선 순위를 따져보면 `ug`를 먼저 합치게 됩니다. 토큰화 결과는 `m`, `ug`이 될텐데요. 이 가운데 `m`은 어휘 집합에 속해 있지 않음을 확인할 수 있습니다. 따라서 `mug`의 최종 토큰화 결과는 `<unk>`, `ug`가 됩니다.

결론적으로 `pug bug mug`라는 문장의 BPE 토큰화 결과는 다음과 같습니다.

- `pug bug mug` > `p`, `ug`, `b`, `ug`, `<unk>`, `ug`

한편 일반적으로 알파벳 등 개별 문자들은 BPE 어휘 구축시 초기 사전에 들어가기 때문에 `mug`의 사례처럼 UNK 토큰이 발생하는 경우는 많지 않은 편입니다. BPE가 어휘 집합 크기를 합리적으로 유지하면서도 어휘 구축시 보지 못했던 단어(신조어 등)에 대해서 유의미한 분절을 수행할 수 있는 배경입니다.

---

## 워드피스

워드피스(wordpiece)는 말뭉치에서 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사합니다. 다만 어휘 집합을 구축할 때 문자열을 병합하는 기준이 다릅니다. WordPiece는 BPE와 같이 가장 많이 등장한 쌍을 병합하는 것이 아니라, 병합되었을 때 코퍼스의 Likelihood를 가장 높이는 쌍을 병합하게 됩니다.

예컨대 BPE 어휘 구축 예시에서 `ug`를 병합하는 상황이라고 가정해 보겠습니다. 워드피스에서는 말뭉치에서 `ug`가 동시에 등장할 확률을 `u`와 `g`가 각각 등장할 확률을 곱한 값으로 나눈 값이 다른 쌍보다 클 경우 해당 쌍을 병합하게 됩니다. 

다시 말해 `u`와 `g`가 서로 독립(각각의 등장이 서로의 등장에 전혀 영향을 주지 않는 상태)임을 가정했을 때 대비 둘이 얼마나 자주 동시에 등장하는지를 따져 본다는 것입니다. 즉, 워드피스에서는 병합 후보에 오른 쌍을 미리 병합해보고 잃게 되는 것은 무엇인지, 해당 쌍을 병합할 가치가 충분한지 등을 종합적으로 판단한 후에 병합을 수행합니다.

토큰화를 수행하는 방식도 BPE와 워드피스가 약간 다릅니다. BPE의 경우 병합 우선순위(`merge.txt`)를 참고해 우선 순위가 높은 바이그램 쌍을 반복적으로 합치는 방식으로 토큰화를 수행하는데요. 워드피스는 어휘 집합(`vocab.txt`)만 가지고 토큰화를 실시합니다. 

워드피스에서는 분석 대상 어절에 어휘 집합에 있는 서브워드가 포함돼 있을 경우 해당 서브워드를 어절에서 분리한다(최장 일치 기준). 이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고, 또 분리합니다. 어절 끝까지 찾았는데 어휘 집합에 없으면 이를 미등록 단어(unknown word)로 취급합니다.

---

## 참고문헌

- [Byte Pair Encoding - Wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding)
- [Huggingface Tokenizer Summary](https://huggingface.co/transformers/master/tokenizer_summary.html)

---